{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "spark://spark-master:7077"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Excercices scala (dans PolyNotes)\n",
        "\n",
        "\n",
        "*Architecture: https://github.com/AndreiArion/bigdata-telecom-docker/blob/main/architecture.png*\n",
        "\n",
        "\n",
        "*Dans votre environnement  vous avez les URLs suivants:*\n",
        "\n",
        "\n",
        "* \n",
        "  Spark Driver (polynote) : [http://localhost:4043](http://localhost:4043/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Master : [http://localhost:9090/](http://localhost:9090/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Worker 1: [http://localhost:9091/](http://localhost:9091/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Worker 2: [http://localhost:9092/](http://localhost:9092/)\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "*Vous pouvez commencer par les exercices Scala ([http://andreiarion.github.io/TP4_TPSpark_RDD.html#excercices-scala](http://andreiarion.github.io/TP4_TPSpark_RDD.html#excercices-scala) )*<br>\n",
        "\n",
        "*Documentation: **[https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)*\n",
        "\n",
        "*<br>*\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669373772281,
          "endTs" : 1669373772519
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val myNumbers = List(1, 2, 5, 4, 7, 3)\n",
        "myNumbers"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669373861847,
          "endTs" : 1669373861979
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def cube(a: Int): Int = a * a * a"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669373869416,
          "endTs" : 1669373869668
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "myNumbers.map(x => cube(x))"
      ],
      "outputs" : [
        {
          "execution_count" : 5,
          "data" : {
            "text/plain" : [
              "List(1, 8, 125, 64, 343, 27)"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "List[Int]"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669373416411,
          "endTs" : 1669373417705
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sparkContext.parallelize(1 to 10).count"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "text/plain" : [
              "10"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Long"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 6,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Exercices RDDs\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669374243747,
          "endTs" : 1669374243902
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val data = 1 to 1000"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669375423612,
          "endTs" : 1669375423811
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val paralelizedData=  spark.sparkContext.parallelize(data)\n",
        "paralelizedData"
      ],
      "outputs" : [
        {
          "execution_count" : 9,
          "data" : {
            "text/plain" : [
              "ParallelCollectionRDD[21] at parallelize at Cell9:1"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "RDD[Int]"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "<div class=\"sect5\" style=\"margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding: 0px; color: rgba(0, 0, 0, 0.8); font-family: &quot;Noto Serif&quot;, &quot;DejaVu Serif&quot;, serif; background-color: rgb(255, 255, 255);\"><h6 id=\"1-2-ecrire-une-courte-séquence-scalaspark-qui-affiche-la-somme-des-nombres-paires-de-1-a-1000-utilisez-la-fonction-fold-disponible-sur-un-rdd-qui-est-similaire-a-foldleft-de-scala\" style=\"margin: 1em 0px 0.5em; padding: 0px; font-family: &quot;Open Sans&quot;, &quot;DejaVu Sans&quot;, sans-serif; font-weight: 300; color: rgb(186, 57, 37); text-rendering: optimizelegibility; line-height: 1.2; font-size: 1em; word-spacing: -0.05em;\">1.2 Ecrire une courte séquence Scala/Spark qui affiche la somme des nombres paires de 1 a 1000 (utilisez la fonction&nbsp;<em style=\"line-height: inherit;\">fold</em>&nbsp;disponible sur un RDD qui est similaire a&nbsp;<a href=\"http://www.arolla.fr/blog/2011/10/listes-scala-methodes-foldleft-et-foldright/\" style=\"background-image: none; background-position: initial; background-size: initial; background-repeat: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: rgb(33, 86, 165); line-height: inherit;\">foldLeft de scala</a>).</h6></div><div class=\"sect5\" style=\"margin: 0px; padding: 0px; color: rgba(0, 0, 0, 0.8); font-family: &quot;Noto Serif&quot;, &quot;DejaVu Serif&quot;, serif; background-color: rgb(255, 255, 255);\"><h6 id=\"1-3-ecrire-une-courte-séquence-scalaspark-qui-affiche-le-produit-des-nombres-paires-de-1-a-1000\" style=\"margin: 1em 0px 0.5em; padding: 0px; font-family: &quot;Open Sans&quot;, &quot;DejaVu Sans&quot;, sans-serif; font-weight: 300; color: rgb(186, 57, 37); text-rendering: optimizelegibility; line-height: 1.2; font-size: 1em; word-spacing: -0.05em;\"><br></h6></div>"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 11,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "<div class=\"sect5\" style=\"margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding: 0px; color: rgba(0, 0, 0, 0.8); font-family: &quot;Noto Serif&quot;, &quot;DejaVu Serif&quot;, serif; background-color: rgb(255, 255, 255);\"><h6 id=\"1-3-ecrire-une-courte-séquence-scalaspark-qui-affiche-le-produit-des-nombres-paires-de-1-a-1000\" style=\"margin: 1em 0px 0.5em; font-size: 1em; font-weight: 300; line-height: 1.2; color: rgb(186, 57, 37); padding: 0px; font-family: &quot;Open Sans&quot;, &quot;DejaVu Sans&quot;, sans-serif; text-rendering: optimizelegibility; word-spacing: -0.05em;\">1.3 Ecrire une courte séquence Scala/Spark qui affiche le produit des nombres paires de 1 a 1000.</h6><div><br></div></div><div class=\"sect5\" style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; padding: 0px; color: rgba(0, 0, 0, 0.8); font-family: &quot;Noto Serif&quot;, &quot;DejaVu Serif&quot;, serif; background-color: rgb(255, 255, 255);\"></div>"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 14,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 12,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "1.4 Combiner les deux derniers programmes en un seul. Mettre en cache les données au niveau des noeuds et vérifier la répartition des ces données via le *Spark UI*"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 17,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 15,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "<font color=\"#ba3925\" face=\"Open Sans, DejaVu Sans, sans-serif\"><span style=\"word-spacing: -0.8px;\">1.5 WordCount: écrire le programme pour compter l’occurence des mots du fichier </span></font>*<font color=\"#ba3925\" face=\"Open Sans, DejaVu Sans, sans-serif\"><span style=\"word-spacing: -0.8px;\">\"/opt/spark/README.md\"</span></font>*."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 18,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 16,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "1.6 Ecrire le programme qui donne le mot le plus souvent utilisé du fichier (vous pouvez utiliser *sortByKey* après avoir inversé la liste des paires (mot,nb_occurences)). Suivez via le SparkUI les stage d’executions de votre traitement ([http://localhost:4040/](http://localhost:4040/))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 20,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## SparkSQL\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 19,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "3.1 Utiliser SparkSQL pour trouver dans le fichier */opt/spark/examples/src/main/resources/people.txt* les noms des personnes qui ont moins de 19 ans"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 22,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669375312899,
          "endTs" : 1669375315126
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sqlContext = spark.sqlContext\n",
        "import sqlContext.sql"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 21,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669375374835,
          "endTs" : 1669375376437
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "\n",
        "case class Person(name: String, age: Int) \n",
        "\n",
        "\n",
        "val people = spark.sparkContext.textFile(\"/opt/spark/examples/src/main/resources/people.txt\").map(_.split(\",\")).map(p => Person(p(0), p(1).trim.toInt)).toDF \n",
        "\n",
        "people.createOrReplaceTempView(\"people\") \n",
        "\n",
        "\n",
        "val teenagers = sql(\"TODO\") \n",
        "\n",
        "\n",
        "teenagers.show "
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|Michael| 29|\n",
            "|   Andy| 30|\n",
            "| Justin| 19|\n",
            "+-------+---+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 23,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669375320724,
          "endTs" : 1669375320786
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}