{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.datastax.spark:spark-cassandra-connector-driver_2.12:3.1.0"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "local[*]",
        "spark.cassandra.connection.host" : "cassandra-1",
        "spark.jars.packages" : "com.datastax.spark:spark-cassandra-connector-driver_2.12:3.1.0"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## Excercices scala (dans PolyNotes)\n",
        "\n",
        "\n",
        "*Architecture: https://github.com/AndreiArion/bigdata-telecom-docker/blob/main/architecture.png*\n",
        "\n",
        "\n",
        "*Dans votre environnement  vous avez les URLs suivants:*\n",
        "\n",
        "\n",
        "* \n",
        "  Spark Driver (polynote) : [http://localhost:4043](http://localhost:4043/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Master : [http://localhost:9090/](http://localhost:9090/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Worker 1: [http://localhost:9091/](http://localhost:9091/)\n",
        "  \n",
        "  \n",
        "  \n",
        "* \n",
        "  Spark Worker 2: [http://localhost:9092/](http://localhost:9092/)\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "*Vous pouvez commencer par les exercices Scala ([http://andreiarion.github.io/TP4_TPSpark_RDD.html#excercices-scala](http://andreiarion.github.io/TP4_TPSpark_RDD.html#excercices-scala) )*<br>\n",
        "\n",
        "*Documentation: **[https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)*\n",
        "\n",
        "*<br>*\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 1,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "<div class=\"cell text_cell col-md-12 rendered selected\" tabindex=\"2\" data-cell-id=\"7CAF9FFB8BCC48F78775AADC22C5845F\" style=\"position: relative; min-height: 1px; padding: 5px; float: left; width: 1650px; border: thin solid rgb(171, 171, 171); -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: flex; flex-direction: row; align-items: stretch; border-radius: 2px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; outline: none;\"><div class=\"inner_cell\" style=\"-webkit-box-orient: vertical; -webkit-box-align: stretch; display: flex; flex-direction: column; align-items: stretch; -webkit-box-flex: 1; flex: 1 1 0%;\"><div class=\"text_cell_render rendered_html\" tabindex=\"-1\" style=\"outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em;\"><h2 id=\"Intro\" style=\"color: inherit; font-family: inherit; font-size: 21.994px; line-height: 1; margin: 0.636em 0px 0px;\">Intro<a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Intro\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: hidden;\"></a></h2><p style=\"margin: 1em 0px 0px;\"><font color=\"#000000\" face=\"Helvetica Neue, Helvetica, Arial, sans-serif\"><span style=\"font-size: 14px;\">Pour les exercices suivant on va utiliser une base de donnees publique du gouvernement qui contient des donnees sur les accidents de la route (</span></font><font color=\"#337ab7\" face=\"Helvetica Neue, Helvetica, Arial, sans-serif\"><span style=\"font-size: 14px;\"><u>https://www.data.gouv.fr/fr/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021&nbsp;</u></span></font><font color=\"#000000\" face=\"Helvetica Neue, Helvetica, Arial, sans-serif\"><span style=\"font-size: 14px;\">allons utiliser des dataframes et datasets pour manipuler ces donnees.</span></font></p><h2 id=\"Environement\" style=\"color: inherit; font-family: inherit; font-size: 21.994px; line-height: 1; margin: 1.27em 0px 0px;\">Environement<a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Environement\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: hidden;\"></a></h2><p style=\"color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px; margin: 1em 0px 0px;\">Vous pouvez faire ces excercices dans le spark shell.</p><h2 id=\"Description-des-donnes\" style=\"color: inherit; font-family: inherit; font-size: 21.994px; line-height: 1; margin: 1.27em 0px 0px;\">Description des donnes<a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Description-des-donnes\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: hidden;\"></a></h2><p style=\"color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px; margin: 1em 0px 0px;\">Pour chaque accident corporel (soit un accident survenu sur une voie ouverte à la circulation publique, impliquant au moins un véhicule et ayant fait au moins une victime ayant nécessité des soins), des saisies d’information décrivant l’accident sont effectuées par l’unité des forces de l’ordre (police, gendarmerie, etc.) qui est intervenue sur le lieu de l’accident.</p><p style=\"color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px; margin: 1em 0px 0px;\">Ces saisies sont rassemblées dans une fiche intitulée bulletin d’analyse des accidents corporels (BAAC). Cela comprend des informations de localisation de l’accident, telles que renseignées ainsi que des informations concernant les caractéristiques de l’accident et son lieu, les véhicules impliqués et leurs victimes.</p><p style=\"color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px; margin: 1em 0px 0px;\">Les bases de données de 2005 à 2015 sont désormais annuelles et composées de 4 fichiers (Caractéristiques – Lieux – Véhicules – Usagers ) au format csv.</p><p style=\"margin: 1em 0px 0px;\"><font color=\"#000000\" face=\"Helvetica Neue, Helvetica, Arial, sans-serif\"><span style=\"font-size: 14px;\">La description des differents fichiers se trouve ici: </span></font><font color=\"#337ab7\" face=\"Helvetica Neue, Helvetica, Arial, sans-serif\"><span style=\"font-size: 14px;\"><u>https://www.data.gouv.fr/fr/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/</u></span></font><span style=\"color: inherit; font-family: inherit; font-size: 21.994px; font-weight: 600;\">Documentation</span></p><h2 id=\"Documentation\" style=\"color: inherit; font-family: inherit; font-size: 21.994px; line-height: 1; margin: 1.27em 0px 0px;\"><a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Documentation\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: hidden;\"></a></h2><p style=\"color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px; margin: 1em 0px 0px;\">Documentation dataframes:&nbsp;<a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes\" target=\"_blank\" style=\"color: rgb(51, 122, 183);\">http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes</a>&nbsp;Documentation API scala:&nbsp;<a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\" target=\"_blank\" style=\"color: rgb(51, 122, 183);\">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset</a></p><h2 id=\"Dans-les-exercices-suivants-remplaceez-les-TODO-par-le-code-necessaire\" style=\"color: inherit; font-family: inherit; font-size: 21.994px; line-height: 1; margin: 1.27em 0px 0px;\"><em>Dans les exercices suivants remplaceez les TODO par le code necessaire</em><a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Dans-les-exercices-suivants-remplaceez-les-TODO-par-le-code-necessaire\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: hidden;\"></a></h2></div></div></div><div class=\"cell text_cell rendered col-md-12 unselected\" tabindex=\"2\" data-cell-id=\"433B1BE1A3A34E1A8B15AD14E4AA6D1B\" style=\"position: relative; min-height: 1px; padding: 5px; float: left; width: 1650px; border: thin solid transparent; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: flex; flex-direction: row; align-items: stretch; border-radius: 2px; margin-top: 0px; margin-right: 0px; margin-left: 0px; outline: none; color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 14px;\"><div class=\"inner_cell\" style=\"-webkit-box-orient: vertical; -webkit-box-align: stretch; display: flex; flex-direction: column; align-items: stretch; -webkit-box-flex: 1; flex: 1 1 0%;\"><div class=\"text_cell_render rendered_html\" tabindex=\"-1\" style=\"outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em;\"><h2 id=\"Lecture-et-exploration-des-donnees-CSV\" style=\"font-family: inherit; line-height: 1; color: inherit; margin: 0.636em 0px 0px; font-size: 21.994px;\">Lecture et exploration des donnees CSV<a class=\"anchor-link\" href=\"http://localhost:9001/notebooks/telecom2016/TP6_Dataframes.snb#Lecture-et-exploration-des-donnees-CSV\" style=\"color: rgb(51, 122, 183); text-decoration-line: none; padding: 0px 20px; visibility: visible;\">¶</a></h2></div></div></div>"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669849422525,
          "endTs" : 1669849423000
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.SparkConf\n",
        "def TODO = ??? // ceci est juste un marquer d'une valeur que vous devez remplacer dans les lignes qui suivent\n",
        "// on cree une session Spark avec un nom particulier pour la retrouver plus facilement dans le SparkUI\n",
        "val mySession = SparkSession\n",
        "  .builder()\n",
        "  .config(new SparkConf()\n",
        "             .setAppName(\"TODO\"))//rajouter comme parametre le nom de votre application\n",
        "  .getOrCreate()"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669849445741,
          "endTs" : 1669849446025
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val path = \"/opt/spark-data/\"\n",
        "// Lecture des donnes dans des DataFrames en utilisant la premier ligne du fichier comme nom des colonnes\n",
        "val lieux /*:DataFrame*/ = mySession.read.option(\"header\",\"true\").format(\"com.databricks.spark.csv\")\n",
        "                                      .load(path + \"/lieux_2021.csv\")\n",
        "                                      .cache\n",
        "val caracteristiques = mySession.read.option(\"header\",\"true\").format(\"com.databricks.spark.csv\")\n",
        "                                       .load(path + \"/caracteristiques_2021.csv\")\n",
        "                                       .cache\n",
        "val usagers = mySession.read.option(\"header\",\"true\").format(\"com.databricks.spark.csv\")\n",
        "                                      .load(path + \"/usagers_2021.csv\")\n",
        "                                      .cache\n",
        "val vehicules = TODO"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.sql.AnalysisException",
          "evalue" : "Path does not exist: file:/opt/spark-data/lieux_2021.csv",
          "traceback" : [
            "org.apache.spark.sql.errors.QueryCompilationErrors$,dataPathNotExistError,QueryCompilationErrors.scala,1011",
            "org.apache.spark.sql.execution.datasources.DataSource$,$anonfun$checkAndGlobPathIfNecessary$4,DataSource.scala,785",
            "org.apache.spark.sql.execution.datasources.DataSource$,$anonfun$checkAndGlobPathIfNecessary$4$adapted,DataSource.scala,782",
            "org.apache.spark.util.ThreadUtils$,$anonfun$parmap$2,ThreadUtils.scala,372",
            "scala.concurrent.Future$,$anonfun$apply$1,Future.scala,659",
            "scala.util.Success,$anonfun$map$1,Try.scala,255",
            "scala.util.Success,map,Try.scala,213",
            "scala.concurrent.Future,$anonfun$map$1,Future.scala,292",
            "scala.concurrent.impl.Promise,liftedTree1$1,Promise.scala,33",
            "scala.concurrent.impl.Promise,$anonfun$transform$1,Promise.scala,33",
            "scala.concurrent.impl.CallbackRunnable,run,Promise.scala,64",
            "java.util.concurrent.ForkJoinTask$RunnableExecuteAction,exec,ForkJoinTask.java,1426",
            "java.util.concurrent.ForkJoinTask,doExec,ForkJoinTask.java,290",
            "java.util.concurrent.ForkJoinPool$WorkQueue,topLevelExec,ForkJoinPool.java,1020",
            "java.util.concurrent.ForkJoinPool,scan,ForkJoinPool.java,1656",
            "java.util.concurrent.ForkJoinPool,runWorker,ForkJoinPool.java,1594",
            "java.util.concurrent.ForkJoinWorkerThread,run,ForkJoinWorkerThread.java,183"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 4,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "3.1 WordCount avec *Spark* et *Cassandra*"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1669848650678,
          "endTs" : 1669848650898
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.datastax.spark.connector._\n",
        "val sc= spark.sparkContext\n",
        "\n",
        "val df = spark\n",
        "    .read\n",
        "    .format(\"org.apache.spark.sql.cassandra\")\n",
        "    .options(Map(\"table\" -> \"fooTable\", \"keyspace\" -> \"bar\")).load.cache()"
      ],
      "outputs" : [
        {
          "ename" : "java.lang.ClassNotFoundException",
          "evalue" : "\nFailed to find data source: org.apache.spark.sql.cassandra. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       ",
          "traceback" : [
            "org.apache.spark.sql.errors.QueryExecutionErrors$,failedToFindDataSourceError,QueryExecutionErrors.scala,587",
            "org.apache.spark.sql.execution.datasources.DataSource$,lookupDataSource,DataSource.scala,675",
            "org.apache.spark.sql.execution.datasources.DataSource$,lookupDataSourceV2,DataSource.scala,725",
            "org.apache.spark.sql.DataFrameReader,load,DataFrameReader.scala,207",
            "org.apache.spark.sql.DataFrameReader,load,DataFrameReader.scala,171",
            "notebook0.Cell2$4,<init>,Cell2,7",
            "jdk.internal.reflect.NativeConstructorAccessorImpl,newInstance0,NativeConstructorAccessorImpl.java,-2",
            "jdk.internal.reflect.NativeConstructorAccessorImpl,newInstance,NativeConstructorAccessorImpl.java,62",
            "jdk.internal.reflect.DelegatingConstructorAccessorImpl,newInstance,DelegatingConstructorAccessorImpl.java,45",
            "java.lang.reflect.Constructor,newInstance,Constructor.java,490",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,createInstance,ScalaInterpreter.scala,161",
            "polynote.kernel.interpreter.scal.ScalaInterpreter,$anonfun$runClass$4,ScalaInterpreter.scala,173",
            "zio.blocking.package$Blocking$Service,$anonfun$effectBlockingInterrupt$5,package.scala,126",
            "zio.ZIO$,$anonfun$effectSuspend$1,ZIO.scala,2782",
            "zio.internal.FiberContext,liftedTree1$1,FiberContext.scala,571",
            "zio.internal.FiberContext,evaluateNow,FiberContext.scala,571",
            "zio.internal.FiberContext,$anonfun$fork$17,FiberContext.scala,770",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$3,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withErr,Console.scala,196",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$2,CellExecutor.scala,34",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "scala.util.DynamicVariable,withValue,DynamicVariable.scala,62",
            "scala.Console$,withOut,Console.scala,167",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,$anonfun$run$1,CellExecutor.scala,33",
            "scala.runtime.java8.JFunction0$mcV$sp,apply,JFunction0$mcV$sp.java,23",
            "polynote.kernel.package$,withContextClassLoader,package.scala,67",
            "polynote.kernel.interpreter.CellExecutor$$anon$1,run,CellExecutor.scala,31",
            "java.util.concurrent.ThreadPoolExecutor,runWorker,ThreadPoolExecutor.java,1128",
            "java.util.concurrent.ThreadPoolExecutor$Worker,run,ThreadPoolExecutor.java,628",
            "java.lang.Thread,run,Thread.java,829"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}